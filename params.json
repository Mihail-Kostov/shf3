{"name":"Shf3","tagline":"General Bash Framework","body":"Shell Framework 3\r\n==============\r\n\r\nShell framework is a Bash 3 based script library designed for fast script development and advanced bash scripting.\r\n\r\nBasics\r\n-------\r\nThe framework is a directory hierarchy which is usually installed into user home. You only need a unix like system. It is tested on Linux and OS X.\r\n\r\nInstallation\r\n-------------\r\n\r\n    cd $HOME\r\n    git clone git://github.com/hornos/shf3.git\r\n\r\nEdit your `.bashrc` or `.profile`:\r\n\r\n    source $HOME/shf3/bin/shfrc\r\n\r\nIf you just want to activate it type the command above into your shell.\r\n\r\n### Update\r\nYou can update the framewrok from Github by:\r\n\r\n    shfmgr -u\r\n\r\n### Backup or relocate\r\nThe framework is self-contained. All you need to do is copy or move the entire `$HOME/shf3` directory to a new location or a new machine or an encrypted external drive. All your SSH keys, GSI certificates, queueu configurations will move.\r\n\r\n\r\nSSH Wrapper\r\n----------------\r\nThe framework contains a sophisticated SSH wrapper. You can use ssh, scp, sshfs and even gsi-ssh in the same manner. The wrapper is independent of the `$HOME/.ssh/config` settings and employs command line arguments only.\r\n\r\nFor every `user@login` pair you have to create a descriptor file where the options and details of the account is stored. Descriptor files are located in `$HOME/shf3/mid/ssh` directory. The name of the file is a meta-id (MID) which refers to options of the descriptor file. Wrapper commands use the MID for each connection. MIDs are unique and short. All you have to remember is the MID. With the SSH wrapper it is easy to have different SSH keys and options for each account. You can create the MID file from `$HOME/shf3/mid/template.ssh` or create it by the manager:\r\n\r\n    sshmgr -n skynet\r\n\r\nOptionally, you can generate private/public key pairs. Keys are located in `$HOME/shf3/key/ssh` directory. Secret key is named `MID.sec`, public key is named `MID.pub`. The public part should be added to `authorized_keys` on remote machines.\r\n\r\n### Options in the MID file\r\nThe MID file is a regular Bash script file, which contains only `key=value` pairs. You can source it from any other script.\r\n\r\n    # ssh for regular ssh, gsi for gsi-ssh (see later) \r\n    mid_ssh_type=ssh\r\n    # FQDN of the remote host\r\n    mid_ssh_fqdn=\"skynet.cyberdy.ne\"\r\n    # remote user name\r\n    mid_ssh_user=\"$USER\"\r\n    # SSH port\r\n    mid_ssh_port=222\r\n    # check port explicitly before connect by nmap\r\n    mid_ssh_port_check=\"nmap\"\r\n    # common SSH options for ssh and scp\r\n    mid_ssh_opt=\r\n    # SSH only options\r\n    mid_ssh_ssh_opt=\r\n    # remote directory where SCP will copy\r\n    mid_ssh_scp_dst=\"/home/${mid_ssh_user}\"\r\n    # SSH tunnel options, eg. proxy redirection\r\n    mid_ssh_tunnel=\"-L63128:localhost:3128\"\r\n    # sshfs remote mount target\r\n    mid_ssh_sshfs_dst=\"/home/${mid_ssh_user}\"\r\n    # sshfs options\r\n    mid_ssh_sshfs_opt=\r\n\r\nLogin to a remote machine is done by:\r\n\r\n    sshto -m MID\r\n\r\nKeys will be used if present. If you have tunnels a lock file is created to prevent duplicated redirection. After a not clean logout lock file remains. To force tunnels against the lock run:\r\n\r\n    sshto -f -m MID\r\n\r\nFile or directory transfer can be done between your CWD and `$mid_ssh_scp_dst` directory. To copy a file/dir from CWD:\r\n\r\n    sshtx put MID FILE/DIR\r\n\r\nTo copy back to CWD from `$mid_ssh_scp_dst` run:\r\n\r\n    sshtx get MID FILE/DIR\r\n\r\nNote that SSH keys, options and port as well as SCP options are used from the MID file. The transfer command employs rsync with ssh therefore it is especially suitable to synchronise large files or directories. Transfers can be interrupted and resumed at will.\r\n\r\nYou can mount the account with sshfs. All you need is the MID and a valid setting of `mid_ssh_sshfs_dst` in the MID file. Accounts are mounted to `$HOME/sshfs/MID`. To mount a MID:\r\n\r\n    sshmnt -m MID\r\n\r\nTo unmount:\r\n\r\n    sshmnt -u MID\r\n\r\nNote that you have to install fuse and sshfs for your OS.\r\n\r\nThe `sshmgr` command can be used to manage your MIDs. You can list your accounts by:\r\n\r\n    sshmgr\r\n\r\nGet info of an account by:\r\n\r\n    sshmgr -i MID\r\n\r\n\r\n### GSI SSH Support\r\nLogin, transfer and mount commands are GSI aware. Grid proxy is initialized and destroyed automatically. Login proxies are separate. Certificates are sperate from your `$HOME/.globus` settings.\r\n\r\nIn order to use GSI you have to include the following options in the MID:\r\n\r\n    mid_ssh_type=gsi\r\n    # Grid proxy timeout in HH:MM format\r\n    mid_ssh_valid=\"12:00\"\r\n    # Grid certificate group\r\n    mid_ssh_grid=\"prace\"\r\n\r\nCurrently, PRACE grid certificates are supported. You can create a grid certificate file in `$HOME/shf3/mid/crt/GRID`, where `GRID` is the name of the grid certificate group. The content of the `GRID` file is:\r\n\r\n    mid_crt_url=\"URL_TO_CERTS.tar.gz\"\r\n\r\nPRACE is supported out-of-the-box but certificates have to be downloaded or updated by:\r\n\r\n    sshmgr -g prace\r\n\r\nCertificates are downloaded to `$HOME/shf3/crt/prace` directory. Your grid account certificate should be copied to `$HOME/shf3/key/ssh/MID.crt` and the secret key (pem file) to `$HOME/shf3/key/ssh/MID.sec` . If you create a Grid account with `sshmgr -n` you have to skip SSH key generation and have to do it manually by the following command:\r\n\r\n    sshkey -n MID -g URL_TO_OPENSSL_CONFIG\r\n\r\nThe certificate configuration is used by openssl to generate the secret key and the certificate request. The request is found in `$HOME/shf3/key/ssh/MID.csr` . Note that the sshkey command calls the shf3 password manager to store challenge and request passwords.\r\n\r\n### Encrypted directories\r\nIf you install FUSE [encfs https://en.wikipedia.org/wiki/EncFS] you can have encripted directories. MID files or encrypted directories are in `$HOME/shf3/encfs` directory. You can create an encfs MID by:\r\n\r\n    encfsmgr -n secret\r\n\r\nThe MID file has to contain one line:\r\n\r\n    # location of the encrypted directory\r\n    mid_encfs_enc=\"${HOME}/.secret\"\r\n\r\nSelect `p` for pre-configured paranoia mode and type your encryption password. Note that encryption keys ar located in `$HOME/shf3/key/encfs` directory (`MID.sec` files) and not in the encrypted directory.\r\n\r\nTo mount the encrypted directory:\r\n\r\n    encfsmnt -m MID\r\n\r\nEncrypted directories are mounted to `$HOME/encfs/MID` . Start to encrypt your secret files by moving stuff into this directory.\r\n\r\nQueue Wrapper\r\n------------------\r\nThe queue wrapper library is designed to make batch submission of parallel programs very easy. First, you have configure a MID file for the queue. This MID file contains parameters which are same for every submission. Keys and values are scheduler dependent. Currently, Slurm, PBS and SGE is supported. Queue files are located in `$HOME/shf3/mid/que directory`. Key value pairs are:\r\n\r\n    # scheduler type: slurm, pbs, sge\r\n    QSCHED=slurm\r\n    # email address for notifications\r\n    QMAILTO=your@email\r\n    # which notifications: abe, ALL\r\n    QMAIL=\r\n    # setup scripts\r\n    QSETUP=\"$HOME/shf3/bin/machines $HOME/shf3/bin/scratch\"\r\n    # ulimits\r\n    QLIMIT=\"ulimit -s unlimited\"\r\n    # exclusive allocation\r\n    QEXCL=\"yes\"\r\n\r\nNote that the setup script `machines` and `scratch` is somewhat mandatory. The former sets the variable `MACHINES` to the hostnames of the allocated nodes. You will need this because MPI implementations do not support every scheduler. The `MACHINES` variable is used by the MPI wrapper functions to specify hostnames for the mpirun command. The latter sets the `SCRATCH` variable to your scratch space. Application Wrapper (see later) needs this. Please check each script if you are in doubt.\r\n\r\nThe purpose of the wrapper is to write job scripts for the supported schedulers. The scheduler dependent key values are:\r\n\r\n    # Slurm account\r\n    QPROJ=\r\n    # Slurm partition\r\n    QPART=\r\n    # Slurm constraints\r\n    QCONST=\"\"\r\n    # SGE queue\r\n    QQUEUE=\r\n    # SGE parallel environment\r\n    QPE=\r\n\r\nYou can find templates in `$HOME/shf3/mid/que/templates` directory.\r\n\r\n### Job Submission\r\nIf the queue MID is ready all you need is a job file. The job file is independent of the scheduler and contains only your resource needs. The queue wrapper is designed for MPI or OMP parallel programs. Co-array Fortran and MPI-OMP hybrid mode is also supported. A typical job file is the following (`jobfile`):\r\n\r\n    # name of the job\r\n    NAME=test\r\n    # refers to the queue file in shf3/mid/que/skynet\r\n    QUEUE=skynet\r\n    #  the program to run\r\n    RUN=\"application args\"\r\n    # parallel type of run, here MPI-only run with SGI MPT\r\n    MODE=mpi/mpt\r\n    # compute resources: 8 nodes with 2 sockets and 4 cores per socket\r\n    NODES=8\r\n    SCKTS=2\r\n    CORES=4\r\n    # 2 hours wall time\r\n    TIME=02:00:00\r\n\r\nTo submit your job (batch submission) run:\r\n\r\n    jobmgr -b jobfile\r\n\r\nThis command will run generic checks and shows a summary table of the proposed allocation like that:\r\n\r\n      QSCHED NODES SCKTS   CORES GPUS override\r\n       slurm     1     2       4    0  \r\n             SLOTS TASKS sockets      \r\n                 8     8       2       \r\n        MODE    np    pn threads\r\n     mpi/mpt     8     8       1\r\n\r\nYou can also check the actual, scheduler dependent job script as well. If you have very special needs it is possible to use the wrapper to generate actual job scripts as good starting point. MPI or OMP options are set in various environment variables. You may ask why so serious with parallel submission? Because the above job script is scheduler free and contains only the necessary information. MPI or OMP parameters are error free and calculated accordingly the following table. \r\n\r\nCurrently, OpenMPI, Intel MPI, SGI MPT, OpenMP and Co-Array Fortran is supported.\r\n\r\n### MPI-OMP hybrid mode\r\nIf you specify `MODE=mpiomp` the wrapper configures SCKTS number of MPI process per node and CORES number of OMP threads per MPI process. You can give any combination of SCKTS and CORES. If your scheduler does not support node based allocation like SGE you may have to specify the total number of job slots per node by:\r\n\r\n    SLTPN=8\r\n\r\nand 8 slots will be allocated per node. If you have a large memory requirement you may have to allocate more cores than MPI processes.\r\n\r\n### Application Wrapper\r\nUsually you want to run a script instead of single program. There is no general solution but you can follow this simple scheme:\r\n\r\n1. Preprocess inputs and copy them to the scratch directory\r\n2. Run the application\r\n* Collect and postprocess outputs (eg. gzip) and move to submit directory (somewhere in your `$HOME`)\r\n\r\nUnfortunately, the details of this scheme is application dependent and you have to write wrapper functions for each application. There is a general wrapper which does not do application specific input/output processing. It is a good starting point to develop new wrappers. The Application Wrapper does not depend on the Queue Wrapper, although the Queue Wrapper detects the Application Wrapper.\r\n\r\nFor the Application Wrapper you need a guide file. This file contains information to configure that 3-step scheme above. A general guide contains (`guide`):\r\n\r\n    # submit dir\r\n    INPUTDIR=\"${PWD}\"\r\n    # scratch dir\r\n    WORKDIR=\"${SCRATCH}/hpl-${USER}-${HOSTNAME}-$$\"\r\n    # result dir, usually the same as submit dir\r\n    RESULTDIR=\"${INPUTDIR}\"\r\n    # application binary and options\r\n    PRGBIN=\"${HPL_BIN}/xhpl\"\r\n    PRGOPT=\"\"\r\n    # data inputs, usually precalculated libraries\r\n    DATADIR=\"\"\r\n    DATA=\"\"\r\n    # main input\r\n    MAIN=\"-hpl.dat\"\r\n    # other inputs, usually for restart\r\n    OTHER=\"\"\r\n    # in case of error outputs are saved\r\n    ERROR=\"save\"\r\n    # patterns for outputs to collect\r\n    RESULT=\"*\"\r\n\r\nThe `*DIR` variables tell where to copy inputs from: `MAIN` and `OTHER` is realtive to `INPUTDIR`. The `DATA` key is application specific and \"relative\" to `DATADIR`. The main input is also application specific and can start with a 1 character operator: `-` input is copied but not included int the argument list, `<` input is stdin redirected, and no operator means the input is put into the argument list. The final command is `$PRGBIN $PRGOPT $MAIN` . The program call can be augmented by the queue and the parallel environment. You can run the application with:\r\n\r\n    runprg -p general -g guide\r\n\r\nThis command creates the scratch, copy inputs, run the application, moves back the results and deletes scratch directory. You can combine the two wrappers by setting `RUN=\"runprg -p general -g guide\"` .\r\n\r\n## A Full Example\r\nLet me present a full example as well introduce guide kernels. In this example \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}