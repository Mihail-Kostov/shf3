{"name":"Shf3","tagline":"General Bash Framework","body":"Shell Framework 3\r\n==============\r\n\r\nShell framework is a Bash 3 based script library designed for fast development and advanced bash scripting.\r\n\r\nInstallation\r\n-------------\r\nthe framework is a directory hierarchy which you should install into your `$HOME` directory:\r\n\r\n    cd $HOME\r\n    git clone git://github.com/hornos/shf3.git\r\n\r\nEnable shf3 by editing your `.bashrc` or `.profile`:\r\n\r\n    source $HOME/shf3/bin/shfrc\r\n\r\nor activate it by typing the above command into your shell.\r\n\r\n### Update\r\nYou can update the framewrok from Github by:\r\n\r\n    shfmgr -u\r\n\r\n### Backup or relocate\r\nThe framework is self-contained. All you need to do is copy or move the entire `$HOME/shf3` directory to a new location or a new machine or an encrypted external drive (eg. USB or flash drive). All your SSH keys, GSI certificates and any other configurations are included.\r\n\r\nSSH Wrapper\r\n----------------\r\nThe framework contains a sophisticated SSH wrapper. You can use ssh, scp, sshfs and even gsi-ssh in the same manner. The wrapper is independent of your `$HOME/.ssh/config` settings and employs command line arguments only.\r\n\r\nFor every `user@login` pair you have to create a descriptor file where the options and details of the account is stored. Descriptor files are located in `$HOME/shf3/mid/ssh` directory. The name of the file is a meta-id (MID) which refers to the account and its options. Wrapper commands use the MID to identify a connection. MIDs are unique and short. All you have to remember is the MID. With the SSH wrapper it is easy to have different SSH keys and options for each account you have. You can create the MID file from a template file located at `$HOME/shf3/mid/template.ssh` or with the manager.\r\n\r\n    sshmgr -n <MID>\r\n\r\nThe manager asks qestions about the connection and lets you edit the MID file and generates SSH keys if you want. Optionally, you can generate keys by hand. Keys are located in the `$HOME/shf3/key/ssh` directory. Secret key is named `<MID>.sec`, public key is named `<MID>.pub`. The public part should be added to `authorized_keys` on remote machines.\r\n\r\n### Options in the MID file\r\nThe MID file is a regular Bash script file, which contains only `key=value` pairs. You can source it from any other script.\r\n\r\n    # ssh for regular ssh, gsi for gsi-ssh (see later) \r\n    mid_ssh_type=ssh\r\n    # FQDN of the remote host\r\n    mid_ssh_fqdn=\"skynet.cyberdy.ne\"\r\n    # remote user name\r\n    mid_ssh_user=\"$USER\"\r\n    # SSH port\r\n    mid_ssh_port=222\r\n    # check mahcine with ping and check port with nmap\r\n    mid_ssh_port_check=\"ping nmap\"\r\n    # common SSH options for ssh and scp\r\n    mid_ssh_opt=\r\n    # SSH only options\r\n    mid_ssh_ssh_opt=\r\n    # remote directory where SCP will copy\r\n    mid_ssh_scp_dst=\"/home/${mid_ssh_user}\"\r\n    # SSH tunnel options, eg. proxy redirection\r\n    mid_ssh_tunnel=\"-L63128:localhost:3128\"\r\n    # sshfs remote mount target\r\n    mid_ssh_sshfs_dst=\"/home/${mid_ssh_user}\"\r\n    # sshfs options\r\n    mid_ssh_sshfs_opt=\r\n\r\n#### Additional hostnames\r\nIt is possible to have multiple names for a remote host if you want to access it via a VPN or TOR. For the VPN you add:\r\n\r\n    mid_vpn_fqdn=\"<VPN IP>\"\r\n\r\nWhere the `<VPN IP>` is the remote address on the VPN. For the TOR you need:\r\n\r\n    mid_tor_fqdn=\"<ONION NAME>.onion\"\r\n    mid_tor_proxy=\"localhost:9050\"\r\n\r\nIn order to use VPN or TOR change `<MID>` to `vpn/<MID>` or `tor/<MID>`, respectively. Note that this notation can be used everywhere insted of a regular MID, so to copy a file over TOR you have to write:\r\n\r\n    sshtx put tor/<MID> <FILE>\r\n\r\nor if you wan to mount the MID over the vpn:\r\n\r\n    sshmnt -m vpn/<MID>\r\n\r\n#### SSH hoping\r\n\r\n#### Login and file transfer\r\nLogin to a remote machine is done by:\r\n\r\n    sshto -m <MID>\r\n\r\nIf you have tunnels a lock file is created to prevent duplicated redirection. After a not clean logout lock file remains. To force tunnels against the lock run:\r\n\r\n    sshto -f -m <MID>\r\n\r\nFile or directory transfer can be done between your CWD and `$mid_ssh_scp_dst` directory. To copy a file/dir from CWD:\r\n\r\n    sshtx put <MID> <FILE or DIR>\r\n\r\nTo copy back to CWD from `$mid_ssh_scp_dst` run:\r\n\r\n    sshtx get <MID> <FILE or DIR>\r\n\r\nThe transfer command employs rsync with ssh and it is especially suitable to synchronise large files or directories. Transfers can be interrupted and resumed at will.\r\n\r\n#### Mount\r\nYou can mount the account with sshfs. All you need is the MID and a valid setting of `mid_ssh_sshfs_dst` in the MID file. Accounts are mounted to `$HOME/sshfs/<MID>`. To mount a MID:\r\n\r\n    sshmnt -m <MID>\r\n\r\nTo unmount:\r\n\r\n    sshmnt -u <MID>\r\n\r\nNote that you have to install fuse and sshfs for your OS.\r\n\r\n#### MID Manager\r\n\r\nThe `sshmgr` command can be used to manage your MIDs. You can list your accounts by:\r\n\r\n    sshmgr\r\n\r\nGet info of an account by:\r\n\r\n    sshmgr -i <MID>\r\n\r\nEdit a MID file:\r\n\r\n    sshmgr -e <MID>\r\n\r\n\r\n### GSI SSH Support\r\nLogin, transfer and mount commands are GSI aware. Grid proxy is initialized and destroyed automatically. Login proxies are separate. Certificates are sperate from your `$HOME/.globus` settings.\r\n\r\nIn order to use GSI you have to include the following options in the MID:\r\n\r\n    mid_ssh_type=gsi\r\n    # Grid proxy timeout in HH:MM format\r\n    mid_ssh_valid=\"12:00\"\r\n    # Grid certificate group\r\n    mid_ssh_grid=\"prace\"\r\n\r\nCurrently, PRACE grid certificates are supported. You can create a grid certificate file in `$HOME/shf3/mid/crt/<GRID>`, where `<GRID>` is the name of the grid certificate group. The content of the `<GRID>` file is:\r\n\r\n    mid_crt_url=\"<URL of the CERTS>.tar.gz\"\r\n\r\nPRACE is supported out-of-the-box but certificates have to be downloaded or updated by:\r\n\r\n    sshmgr -g prace\r\n\r\nCertificates are downloaded to `$HOME/shf3/crt/prace` directory. Your grid account certificate should be copied to `$HOME/shf3/key/ssh/<MID>.crt` and the secret key (pem file) to `$HOME/shf3/key/ssh/<MID>.sec` . If you create a Grid account with `sshmgr -n` you have to skip SSH key generation and have to do it manually by the following command:\r\n\r\n    sshkey -n <MID> -g <URL to OPENSSL CONFIG>\r\n\r\nThe certificate configuration is used by openssl to generate the secret key and the certificate request. The request is found in `$HOME/shf3/key/ssh/<MID>.csr` . Note that the sshkey command calls the shf3 password manager to store challenge and request passwords.\r\n\r\n### Password manager\r\nShf3 has a basic shell based password manager. Passwords are stored in an sqlite database and encrypted by GPG. Passwords are random strings from random.org and protected by a per password master password. To generate a 21 character long new password:\r\n\r\n     passmgr -l 21 -u <ID>\r\n\r\nRetrive the generated password:\r\n\r\n    passmgr -u <ID>\r\n\r\nNote that the password is obscured and shown between `>` and `<` caharacters and you have to highlight it to reveal.\r\n\r\nSearch for an ID:\r\n\r\n    passmgr -s <ID>\r\n\r\nList all passwords:\r\n\r\n    passmgr\r\n\r\nPasswords are stored in `$HOME/shf3/sql/enc_pass.sqlite` SQLite database.\r\n\r\n### Encrypted directories\r\nIf you install FUSE [encfs](https://en.wikipedia.org/wiki/EncFS) you can have encripted directories. MID files or encrypted directories are in `$HOME/shf3/encfs` directory. You can create an encfs MID by:\r\n\r\n    encfsmgr -n secret\r\n\r\nThe MID file has to contain one line:\r\n\r\n    # location of the encrypted directory\r\n    mid_encfs_enc=\"${HOME}/.secret\"\r\n\r\nSelect `p` for pre-configured paranoia mode and type your encryption password. Note that encryption keys ar located in `$HOME/shf3/key/encfs` directory (`<MID>.sec` files) and not in the encrypted directory.\r\n\r\nTo mount the encrypted directory:\r\n\r\n    encfsmnt -m <MID>\r\n\r\nEncrypted directories are mounted to `$HOME/encfs/<MID>` . Start to encrypt your secret files by moving stuff into this directory.\r\n\r\nQueue Wrapper\r\n------------------\r\n\r\nShf3 is not a grid middleware tool and does not have middleware support right now. The only requirement is a properly configured scheduler. You have to read and understand the description of the site specific manual of the scheduler. You have to put constant parameters to a queue MID file. Shf3 queue wrapper helps regular HPC users if they can't use a middleware yet they want some interoperatibility. It is specially suitable for groups where job scripts are shared and version controlled.\r\n\r\nShf3 has no support for UNICORE. However, it has some middleware like features. The only requirement is a local job scheduler. Shf3 is for the local batch system. It provides unified local access on the local batch system level. Lot of users are still and will be local batch users using applications where UNICORE is not applicable.\r\n\r\nThe main goal is to have simple and portable job scripts as well as workflow like application scripts. At first for every site you have to create a queue MID. For every job you have to create a job file. Actual job scripts are generated acoording to the queue and job file. If you move to an other machine you just move your shf3 directory, configure a new queue MID and submit your jobs with the new queue.\r\n\r\nThe queue wrapper library is designed to make batch submission of parallel programs very easy. First, you have to configure a MID file for the queue. This MID file contains parameters which are same for every submission. Keys and values are scheduler dependent. Currently, Slurm, PBS and SGE is supported. Queue files are located in `$HOME/shf3/mid/que` directory. The following key/value pairs are common for evry shceduler:\r\n\r\n    # scheduler type: slurm, pbs, sge\r\n    QSCHED=slurm\r\n    # email address for notifications\r\n    QMAILTO=your@email\r\n    # which notifications: abe, ALL\r\n    QMAIL=\r\n    # setup scripts\r\n    QSETUP=\"$HOME/shf3/bin/machines $HOME/shf3/bin/scratch\"\r\n    # ulimits\r\n    QLIMIT=\"ulimit -s unlimited\"\r\n    # exclusive allocation\r\n    QEXCL=\"yes\"\r\n\r\nNote that the setup script `machines` and `scratch` is somewhat mandatory. The former sets the variable `MACHINES` to the hostnames of the allocated nodes. You will need this because MPI implementations do not support every scheduler. The `MACHINES` variable is used by the MPI wrapper functions to specify hostnames for the `mpirun` command. The latter sets the `SCRATCH` variable to your scratch space. Application Wrapper (see later) needs this. Please check each script if you are in doubt. Prace scratch environment variable (`PRACE_SCRATCH`) is supported.\r\n\r\nThe purpose of the wrapper is to write job scripts for each scheduler. The scheduler dependent key/value pairs are:\r\n\r\n    # Slurm account\r\n    QPROJ=\r\n    # Slurm partition\r\n    QPART=\r\n    # Slurm constraints\r\n    QCONST=\"\"\r\n    # SGE queue\r\n    QQUEUE=\r\n    # SGE parallel environment\r\n    QPE=\r\n\r\nYou can find templates in `$HOME/shf3/mid/que/templates` directory.\r\n\r\n### Job Submission\r\nIf the queue MID is ready all you need is a job file. The job file is independent of the scheduler and contains only your resource needs. The queue wrapper is designed for MPI or OMP parallel programs. Co-array Fortran and MPI-OMP hybrid mode is also supported. A typical job file is the following (`jobfile`):\r\n\r\n    # name of the job\r\n    NAME=test\r\n    # refers to the queue file in shf3/mid/que/skynet\r\n    QUEUE=skynet\r\n    # the program to run\r\n    RUN=\"<APP> <ARGS>\"\r\n    # parallel type of run, here MPI-only run with SGI MPT\r\n    MODE=mpi/mpt\r\n    # compute resources: 8 nodes with 2 sockets and 4 cores per socket\r\n    NODES=8\r\n    SCKTS=2\r\n    CORES=4\r\n    # 2 hours wall time\r\n    TIME=02:00:00\r\n\r\nThe `QUEUE` refers to the queue MID file (note that it is not real queue of a system). The `RUN` key sets your application, which can be a script if you need complex pre or post-processing. Please note that you ''do not have to explicitly include mpirun'' before your application, the job wrapper does it. Three modes are supported:\r\n\r\n1. MPI-only (`MODE=mpi/<MPI>`)\r\n2. MPI-OMP hybrid (`MODE=mpiomp/<MPI>`)\r\n3. OpenMP (`MODE=omp`)\r\n4. Co-Array Fortran(`MODE=caf`)\r\n\r\nwhere `<MPI>` is `opmi` (OpenMPI), `ipmi` (Intel MPI), `mpt` (SGI MPT). MPI parameters and OMP settings are generated according to the `NODES`, `SCKTS` and `CORES` resource needs, which are number of nodes, number of CPU sockets per node, and number of CPU cores per socket, respectively. The following table is used to determine the parameters:\r\n\r\n    Par. Mode        # of MPI procs          # of MPI procs/node  # of OMP threads/MPI proc.\r\n    MPI-only (mpi)   NODES × SCKTS × CORES   SCKTS × CORES        1\r\n    OMP-only (omp)   --                      --                   SCKTS × CORES\r\n    MPI-OMP (mpiomp) NODES × SCKTS           SCKTS                CORES\r\n\r\nSubmit your job file:\r\n\r\n    jobmgr -b jobfile\r\n\r\nThis command will run generic checks and shows a summary table of the proposed allocation like that:\r\n\r\n      QSCHED NODES SCKTS   CORES GPUS override\r\n       slurm     1     2       4    0  \r\n             SLOTS TASKS sockets      \r\n                 8     8       2       \r\n        MODE    np    pn threads\r\n     mpi/mpt     8     8       1\r\n\r\nYou can also check the actual, scheduler dependent, job script as well. If you have special needs it is possible to use the wrapper to generate actual job script. Currently, OpenMPI, Intel MPI, SGI MPT, OpenMP and Co-Array Fortran is supported.\r\n\r\n### MPI-OMP hybrid mode\r\nIf you specify `MODE=mpiomp` the wrapper configures `SCKTS` number of MPI process per node and `CORES` number of OMP threads per MPI process. You can give any combination of `SCKTS` and `CORES` values. If your scheduler does not support node based allocation like SGE you may have to specify the total number of job slots per node by:\r\n\r\n    SLTPN=8\r\n\r\nIn this case 8 slots are allocated per node. You can use the hybrid mode if want to run a large memory job on low memory nodes by overallocating.\r\n\r\n### Application Wrapper\r\nIn practice you want to run a script instead of single program. There is no general solution but you can follow this simple Prepare-Run-Collect (PRC) scheme:\r\n\r\n1. Preprocess inputs and copy them to the scratch directory\r\n2. Run the application\r\n* Collect and postprocess outputs (eg. gzip) and move to submit directory (somewhere in your `$HOME`)\r\n\r\nThe details of this scheme is application dependent and you have to write wrapper functions for each application. In shf3 there is a general wrapper which does not do application specific input/output processing yet follows this scheme. It is a good starting point to develop new wrappers. The application wrapper does not depend on the queue wrapper, although the queue wrapper detects the application wrapper.\r\n\r\nThe application wrapper needs a guide file. This file contains information on how to go throught the 3-step scheme. A general guide file contains (`guide`) the following lines:\r\n\r\n    # submit dir\r\n    INPUTDIR=\"${PWD}\"\r\n    # scratch dir\r\n    WORKDIR=\"${SCRATCH}/hpl-${USER}-${HOSTNAME}-$$\"\r\n    # result dir, usually the same as submit dir\r\n    RESULTDIR=\"${INPUTDIR}\"\r\n    # application binary and options\r\n    PRGBIN=\"${HPL_BIN}/xhpl\"\r\n    PRGOPT=\"\"\r\n    # data inputs, usually precalculated libraries\r\n    DATADIR=\"\"\r\n    DATA=\"\"\r\n    # main input\r\n    MAIN=\"-hpl.dat\"\r\n    # other inputs, usually for restart\r\n    OTHER=\"\"\r\n    # in case of error outputs are saved\r\n    ERROR=\"save\"\r\n    # patterns for outputs to collect\r\n    RESULT=\"*\"\r\n\r\nThe `*DIR` variables tell where to copy inputs from: `MAIN` and `OTHER` is realtive to `INPUTDIR`. The `DATA` key is application specific and \"relative\" to `DATADIR`. The main input is also application specific and can start with a 1 character operator: `-` input is copied but not included int the argument list, `<` input is stdin redirected, and no operator means the input is put into the argument list. The final command is `$PRGBIN $PRGOPT $MAIN` . The program call can be augmented by the queue and the parallel environment. You can run the application with:\r\n\r\n    runprg -p general -g guide\r\n\r\nThis command creates the scratch, copy inputs, run the application, moves back the results and deletes scratch directory. You can combine the two wrappers by setting `RUN=\"runprg -p general -g guide\"` .\r\n\r\n## A Full Example\r\nLets assume we have a cluster named Skynet and we needd access and want to run a job on the machine. Install shf3 on your local machine. First we create an SSH MID for the login:\r\n\r\n    sshmgr -n skynet\r\n\r\nThis command generates keys as well. All you need to do is send the public key to Skynet's administrator. The public key is located in `$HOME/shf3/key/ssh/skynet.pub`. Login to the remote machine:\r\n\r\n    sshto -m skynet\r\n\r\nInstall shf on skynet as well. You need this for the job submission. First you have to setup the queue. On this example machine the job shceduler is Slurm. Create a queue file `$HOME/shf3/mid/que/skynet`:\r\n\r\n    QSCHED=\"slurm\"\r\n    QMAILTO=\"bob@gmail.com\"\r\n    QMAIL=\"ALL\"\r\n    QPROJ=\"P-20130320\"\r\n    QCONST=\"ib\"\r\n    QPART=\"batch\"\r\n    QSETUP=\"$HOME/shf3/bin/machines $HOME/shf3/bin/scratch\"\r\n    QULIMIT=\"ulimit -s unlimited; ulimit -l unlimited\"\r\n\r\nAll email notification is enabled. The `P-20130320` Slurm account, the `batch` partition and the `ib` Slurm constraint is used. The job script uses two auxiliary scripts (`machines` and `scratch`). The former determines mahcines for the job, the latter sets the scratch space.\r\n\r\nLets assume we want to run a Viennese application VASP. VASP has full support in shf3. You need have the following job file (`vasp.job`):\r\n\r\n    NAME=TEST\r\n    TIME=12:00:00\r\n    NODES=2\r\n    SCKTS=2\r\n    CORES=4\r\n    QUEUE=skynet\r\n    MODE=\"mpi/mpt\"\r\n    BIND=\"dplace -s 1\"\r\n    RUN=\"runprg -p vasp -g vasp.guide\"\r\n\r\nThe name of the job is `TEST` and will run for 12 hours on 2 nodes and 8 cores per node. The queue is `skynet` which refers to the parameters defined in `$HOME/shf3/mid/que/skynet`. The job will run in MPI-only mode and will use the `dplace -s 1` command for CPU binding. In the last line you define the command to run. Here it is the application wrapper for vasp which needs the guide file (`vasp.guide`).\r\n\r\nThe guide file contains the folllowing lines:\r\n\r\n    INPUTDIR=\"${PWD}\"\r\n    WORKDIR=\"${SCRATCH}/vasp-${USER}-${HOSTNAME}-$$\"\r\n    RESULTDIR=\"${INPUTDIR}\"\r\n    PRGBIN=\"${HOME}/vasp/bin/vasp.mpi\"\r\n    PRGOPT=\"\"\r\n    DATADIR=\"${VASP_PROJ_HOME}/POTCAR_PBE\"\r\n    DATA=\"A B\"\r\n    MAIN=\"-test.cntl\"\r\n    OTHER=\"test.WAVECAR.gz\"\r\n    ERROR=\"save\"\r\n    RESULT=\"*\"\r\n\r\nInput files copied for the current directory and copied to the `WORKDIR` which is defined be the scratch variable. Results will copied back to the current directory. Old results are saved and compressed by default. The `${HOME}/vasp/bin/vasp.mpi` program will run without any arguments. The `MAIN` variable defines the main input files. If you put `-` as the first character the input is not included in the argument list of the program. Auxiliary data libraries come from `DATADIR` according to `DATA`. An other input is also copied to the scratch space. Other input files are copied and uncompressed as is. All the result files will copied back (`RESULT=*`).\r\n\r\nSend your job by:\r\n\r\n    jobmgr -b vasp.job\r\n\r\nThe actual job file will look like this:\r\n\r\n    #!/bin/bash\r\n    ### Date Wed Mar 20 10:32:26 CET 2013\r\n    \r\n    ### Scheduler  : slurm\r\n    #SBATCH --job-name TEST\r\n    #SBATCH --mail-type=ALL\r\n    #SBATCH --mail-user=bob@gmail.com\r\n    #SBATCH --time=12:00:00\r\n    #SBATCH --nodes=2\r\n    #SBATCH --ntasks-per-node=8\r\n    #SBATCH --constraint=ib\r\n    #SBATCH --partition=batch\r\n    #SBATCH -o StdOut\r\n    #SBATCH -e ErrOut\r\n    \r\n    ### Resource Allocation\r\n    #       QSCHED NODES SCKTS   CORES GPUS override\r\n    #        slurm     2     2       4    0  \r\n    #              SLOTS TASKS sockets      \r\n    #                 16     8       4       \r\n    \r\n    ### Queue Setup\r\n    source /home/bob/shf3/bin/machines\r\n    source /home/bob/shf3/bin/scratch\r\n    ulimit -s unlimited; ulimit -l unlimited\r\n    \r\n    ### MPI Setup\r\n    export MPI_MODE=\"mpi\"\r\n    export MPI_MPI=\"mpt\"\r\n    export MPI_MAX_SLOTS=16\r\n    export MPI_MAX_TASKS=8\r\n    export MPI_NUM_NODES=2\r\n    export MPI_NPPN=\"8\"\r\n    export NODES=2\r\n    export SCKTS=2\r\n    export CORES=4\r\n    export BIND=\"dplace -s 1\"\r\n    export MPI_OPT=\"   ${MACHINES} @{MPI_NPPN} ${NUMA} @{BIND}  \"\r\n    export PRERUN=\"mpirun ${MPI_OPT}\"\r\n    \r\n    ### Fortran Multi-processing\r\n    export FMP_MPI_NODES=2\r\n    export FMP_MPI_PERNODE=8\r\n    \r\n    ### OMP Setup\r\n    export OMP_NUM_THREADS=1\r\n    export MKL_NUM_THREADS=1\r\n    export KMP_LIBRARY=serial\r\n    \r\n    ### Parallel Mode\r\n    #         MODE    np    pn threads\r\n    #      mpi/mpt    16     8       1 \r\n\r\n    ### Command\r\n    runprg -p vasp -g vasp.guide -s slurm\r\n\r\n#### Note on `MPI_OPT`\r\n\r\nThis variable is evaluated by `runprg` on-the-fly since you can run a script function, called kernel, instead of the application specified in the guide. The kernel function will call your application an can change MPI and OMP parameters or restart your application during the run. See Workflow scripts section. If you do not use `runprg` the `MPI_OPT` variable does not contain `@` characters and you have a normal prerun line in your actual job script:\r\n\r\n    $PRERUN <YOUR APP>\r\n\r\nLets assume that your colleague works on an other machine called budapest. She wants to reproduce your results and rerun the same calculation. She installed shf3 and configured the following queue MID on the budapest machine (`$HOME/shf3/mid/que/budapest`):\r\n\r\n    QSCHED=\"sge\"\r\n    QMAILTO=\"alice@gmail.com\"\r\n    QMAIL=\"abe\"\r\n    QQUEUE=\"budapest.q\"\r\n    QPE=\"mpi\"\r\n    QEXCL=\"yes\"\r\n    QSHELL=\"/bin/bash\"\r\n    QULIMIT=\"ulimit -s unlimited\"\r\n    QOPT=\"-cwd -V\"\r\n    QSETUP=\"${HOME}/shf3/bin/machines ${HOME}/shf3/bin/scratch\"\r\n\r\nOn that machine the scheduler is SGE and different parameters have to be used. You have to send the following files to Alice:\r\n\r\n    vasp.job\r\n    vasp.guide\r\n    (other application specific inputs)\r\n\r\nSince the budapest machine have a different architecture Alice changes the following lines in `vasp.job`:\r\n\r\n    SCKTS=2\r\n    CORES=12\r\n    MODE=mpi/ompi\r\n    BIND=\"--bycore --bind-to-core\"\r\n\r\nShe switches to OpenMPI and more cores. Every other parameter remains the same. Alice submits the job by the same command:\r\n\r\n    jobmgr -b vasp.job\r\n\r\nShe will generate and submit the following actual job script:\r\n\r\n    #!/bin/bash\r\n    ### Date Wed Mar 20 12:15:50 CET 2013\r\n    \r\n    ### Scheduler  : sge\r\n    #$ -N TEST\r\n    #$ -S /bin/bash\r\n    #$ -m abe\r\n    #$ -M alice@gmail.com\r\n    #$ -l h_cpu=12:00:00\r\n    #$ -pe mpi 48\r\n    #$ -q budapest.q\r\n    #$ -l exclusive=true\r\n    #$ -o StdOut\r\n    #$ -e ErrOut\r\n    #$ -cwd -V\r\n    \r\n    ### Resource Allocation\r\n    #       QSCHED NODES SCKTS   CORES GPUS override\r\n    #          sge     2     2      12    0  \r\n    #              SLOTS TASKS sockets      \r\n    #                 48    24       4       \r\n    \r\n    ### Queue Setup\r\n    source /home/alice/shf3/bin/machines\r\n    source /home/alice/shf3/bin/scratch\r\n    ulimit -s unlimited\r\n    \r\n    ### MPI Setup\r\n    export MPI_MODE=\"mpi\"\r\n    export MPI_MPI=\"ompi\"\r\n    export MPI_MAX_SLOTS=48\r\n    export MPI_MAX_TASKS=24\r\n    export MPI_NUM_NODES=2\r\n    export MPI_NPPN=\"-np 48 -npernode 24\"\r\n    export NODES=2\r\n    export SCKTS=2\r\n    export CORES=12\r\n    export BIND=\"--bycore --bind-to-core\"\r\n    export MPI_OPT=\"    @{MPI_NPPN} ${NUMA} @{BIND}  \"\r\n    export PRERUN=\"mpirun ${MPI_OPT}\"\r\n    \r\n    ### Fortran Mulit-processing\r\n    export FMP_MPI_NODES=2\r\n    export FMP_MPI_PERNODE=24\r\n    \r\n    ### OMP Setup\r\n    export OMP_NUM_THREADS=1\r\n    export MKL_NUM_THREADS=1\r\n    export KMP_LIBRARY=serial\r\n    \r\n    ### Parallel Mode\r\n    #         MODE    np    pn threads\r\n    #     mpi/ompi    48    24       1 \r\n    \r\n    ### Command\r\n    runprg -p vasp -g vasp.guide -s sge\r\n\r\n### HPL test example\r\nHPL is a supported application. You have the following `hpl.job` file:\r\n\r\n    NAME=xhpl\r\n    TIME=06:00:00\r\n    NODES=1\r\n    SCKTS=2\r\n    CORES=12\r\n    QUEUE=budapest\r\n    MODE=mpi/ompi\r\n    BIND=\"--bycore --bind-to-core\"\r\n    RUN=\"runprg -p hpl -g hpl.guide\"\r\n\r\nand the guide file (`hpl.guide`):\r\n\r\n    INPUTDIR=\"${PWD}\"\r\n    WORKDIR=\"${SCRATCH}/hpl-${USER}-${HOSTNAME}-$$\"\r\n    RESULTDIR=\"${INPUTDIR}\"\r\n    PRGBIN=\"${PWD}/xhpl\"\r\n    PRGOPT=\"\"\r\n    DATADIR=\"\"\r\n    DATA=\"\"\r\n    MAIN=\"-HPL.dat\"\r\n    ERROR=\"save\"\r\n    RESULT=\"*\"\r\n\r\n## Workflow scripts\r\nProgram wrapper is a simple workflow manager. It has support for some specific application. You can use the general wrapper to run any kind of application in the PRC scheme. If you want to run the HPL test adove with the general runner (`hpl.job`):\r\n\r\n    NAME=xhpl\r\n    TIME=06:00:00\r\n    NODES=1\r\n    SCKTS=2\r\n    CORES=12\r\n    QUEUE=budapest\r\n    MODE=mpi/ompi\r\n    BIND=\"--bycore --bind-to-core\"\r\n    RUN=\"runprg -p general -g hpl.guide\"\r\n\r\nThe general runner checks only for the `PRGBIN`. In case of supported apps input files are also checked. By using an application specific wrapper you can reduce faulty submissions considerably.\r\n\r\n### Workflow kernel\r\nYou can run a so-called kernel function insted of `PRGBIN` if you specify the following line in the guide file:\r\n\r\n    KERNEL=\"${INPUTDIR}/kernel\"\r\n\r\nThe kernel file is copied to the scratch space and sourced by `runpgr`. The most simple kernel file looks like this (`kernel`):\r\n\r\n    function general/kernel() {\r\n      run/prg/step\r\n    }\r\n\r\nThis kernel runs your application. The kernel functions lives inside `runpgr` you have to be careful what you do here, although, you can do pretty much anything: modify inputs, restart the application, reconfigure parallel paremeters etc. For example if you want to switch to MPI OMP mode on the fly after you do this:\r\n\r\n    function general/kernel() {\r\n      # run the first step\r\n      run/prg/step\r\n      \r\n      # check outputs and modify inputs\r\n      \r\n      # set new socket and core per node parameters\r\n      SCKTS=4\r\n      CORES=2\r\n      BIND=\"omplace -s 1\"\r\n\r\n      # switch to MPI-OMP mode\r\n      run/prg/mode mpiomp\r\n\r\n      # rerun the application\r\n      run/prg/step\r\n    }\r\n\r\nYou can consider kernels as dynamic applications inside the RPC scheme. It is especially usefule if you have simple workflows eg. you have to call the same application with different inputs in sequence. You can save time and imporove utilization by grouping tightly coupled run steps into a kernel. Do more and submit once!\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}