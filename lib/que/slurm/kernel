function que/slurm/constraints() {
  echo $* | sed s/,/\&/g
}

function que/slurm() {
  local _sub="${1:-BATCH}"
  local _bat="${2:-slurm.batch.sh}"

  if [ "${_sub}" = "LOGIN" ] ; then
    _bat="${2:-slurm.login.sh}"
  fi
  if [ ! -r "${_bat}" ] ; then
    msg/error "($FUNCNAME) File not found ${_bat}"
    return ${_false_}
  fi


  # ********************************************************
  #     ____                  __  __
  #    / __ \____  ________  / /_/ /_____ _
  #   / /_/ / __ \/ ___/ _ \/ __/ __/ __ `/
  #  / _, _/ /_/ (__  )  __/ /_/ /_/ /_/ /
  # /_/ |_|\____/____/\___/\__/\__/\__,_/
  #
  # ********************************************************

  # Description          | Input file
  # ---------------------+-------------------------
  # Scheduler queue      | Queue file
  local _jobqueue=${JOBQUEUE:-${QPART}}

  # Node Count           | Job file
  local _nodes=${NODES}

  # CPU Count            | Job file
  local _cpu_slots=${CPU_SLOTS}

  # Wall Clock Limit     | Job file
  local _walltime=${WALLTIME:-${TIME}}

  # Standard Output FIle | job file
  local _stdout=${STDOUT}

  # Standard Error File  | job file
  local _stderr=${STDERR}

  # Copy Environment     | Queue file
  local _copy_env=${COPY_ENV:-no}

  # Event Notification   | Queue file
  local _events=${EVENTS:-${QMAIL}}

  # Email Address        | Queue file
  local _mailto=${MAILTO:-${QMAILTO}}

  # Job Name             | Job file
  local _jobname=${JOBNAME:-${NAME}}

  # Job Restart          | job file
  local _restart=${RESTART:-no}

  # Working Directory    | job file
  local _workdir=${WORK_DIRECTORY}

  # Resource Sharing     | Job file
  local _exclusive=${EXCLUSIVE:-no}

  # Memory Size per core | job file
  local _memory=${MEMORY}

  # Account to charge    | Queue file
  local _account=${ACCOUNT:-${QPROJ}}

  # Tasks Per Node       | Job file
  local _tasks=${TASKS}

  # CPUs per task        | Job file
  local _taskcpus=${TASK_CPUS}

  # Job Dependency       | Job file
  local _depend=${DEPEND}

  # Job Project          | Job file
  local _project=${PROJECT}

  # Job host preference  | Job file
  local _include_nodes=${INCLUDE_NODES}
  local _exclude_nodes=${EXCLUDE_NODES}

  # Quality Of Service   | Job file
  local _quality=${QUALITY}

  # Job Arrays           | Job file
  local _arrays=${ARRAYS}

  # Generic Resources    | Job file
  local _gpus=${GPUS}

  # Licenses             | Job file
  local _licenses=${LICENSES}

  # Begin Time           | job file
  local _begin=${BEGIN}

  # Constraints          | Queue file
  local _constraints=${CONSTRAINTS:-${QCONST}}

  # Other Options        | Queue file
  local _options=${OPTIONS:-${QOPT}}


  # ********************************************************
  #     ____      __                       __  _
  #    /  _/___  / /____  _________ ______/ /_(_)   _____
  #    / // __ \/ __/ _ \/ ___/ __ `/ ___/ __/ / | / / _ \
  #  _/ // / / / /_/  __/ /  / /_/ / /__/ /_/ /| |/ /  __/
  # /___/_/ /_/\__/\___/_/   \__,_/\___/\__/_/ |___/\___/
  #
  # ********************************************************

  if [ "${_sub}" = "LOGIN" ] ; then
    # check the cluster login app
    local _qlogin="${que_slurm_qlogin}"
    if ! u/installed "${_qlogin}" ; then
       msg/error "($FUNCNAME) Not found ${_qlogin}"
       return ${_false_}
    fi

    # Queue
    if ! empty "${_jobqueue}" ; then
      _qlogin="${_qlogin} -p ${_jobqueue}"
    fi

    # Node Count
    if ! empty "${_nodes}" ; then
      _qlogin="${_qlogin} -N ${_nodes}"
    fi

    # CPU Count
    if ! empty "${_cpu_slots}" ; then
      _qlogin="${_qlogin} -n ${_cpu_slots}"
    fi

    # Wall Clock Limit
    if ! empty "${_walltime}" ; then
      _qlogin="${_qlogin} -t ${_walltime}"
    fi

    # N/A Standard Output File
    # N/A Standard Error File

    # Copy Environment
    if [ "${_copy_env}" = "yes" ] ; then
      _qlogin="${_qlogin} --export=ALL"
    fi

    # N/A Event Notification
    # N/A Email Address

    # Job Name
    _qlogin="${_qlogin} --job-name ${_jobname}"

    # N/A Job Restart

    # Working Directory
    if ! empty "${_workdir}" ; then
      _qlogin="${_qlogin} --workdir=${_workdir}"
    fi

    # Resource Sharing
    if [ "${_exclusive}"  = "yes" ] ; then
      _qlogin="${_qlogin} --exclusive"
    fi

    # Memory Size in MB
    if ! empty "${_memory}" ; then
      _qlogin="${_qlogin} --mem-per-cpu=${_memory}M"
    fi

    # Account to charge
    if ! empty "${_account}" ; then
      _qlogin="${_qlogin} --account=${_account}"
    fi

    # Tasks Per Node
    if ! empty "${_tasks}" ; then
      _qlogin="${_qlogin} --ntasks-per-node=${_tasks}"
    fi

    # CPUs Per Task
    if ! empty "${_taskcpus}" ; then
      _qlogin="${_qlogin} --cpus-per-task=${_taskcpus}"
    fi

    # N/A Job Dependency

    # Job Project
    if ! empty "${_project}" ; then
      _qlogin="${_qlogin} --wckey=${_project}"
    fi

    # Job host preference
    if ! empty "${_include_nodes}" ; then
      # TODO: filter spaces
      _qlogin="${_qlogin} --nodelist=${_include_nodes}"
    fi

    if ! empty "${_exclude_nodes}" ; then
      # TODO: filter spaces
      _qlogin="${_qlogin} --exclude=${_exclude_nodes}"
    fi

    # Quality Of Service
    if ! empty "${_quality}" ; then
      _qlogin="${_qlogin} --qos=${_quality}"
    fi

    # Job Arrays
    if ! empty "${_arrays}" ; then
      _qlogin="${_qlogin} --array=${_arrays}"
    fi

    # Generic Resources
    if ! empty "${_gpus}" ; then
      _qlogin="${_qlogin} --gres=gpu:${gpus}"
    fi

    # Licenses
    if ! empty "${_licenses}" ; then
      _qlogin="${_qlogin} --licenses=${_licenses}"
    fi

    # N/A Begin Time

    # Constraints
    if ! empty "${_constraints}" ; then
      local _cons=$(que/slurm/constraints "${_constraints}")
      _qlogin="${_qlogin} --constraint=${_const}"
    fi

    # Options
    if ! empty "${_options}" ; then
      _qlogin="${_qlogin} ${_options}"
    fi

    echo "${_qlogin}" >> "${_bat}"
    return ${_true_}
  fi
  # end Interactive Login


  # ********************************************************
  #     ____        __       __
  #    / __ )____ _/ /______/ /_
  #   / __  / __ `/ __/ ___/ __ \
  #  / /_/ / /_/ / /_/ /__/ / / /
  # /_____/\__,_/\__/\___/_/ /_/
  #
  # ********************************************************

  local _qbatch="${que_slurm}"

  # Queue
  if ! empty "${_jobqueue}" ; then
    echo "#${_qbatch} -p ${_jobqueue}" \
    >> "${_bat}"
  fi

  # Node Count
  if ! empty "${_nodes}" ; then
    echo "#${_qbatch} -N ${_nodes}" \
    >> "${_bat}"
  fi

  # CPU Count
  if ! empty "${_cpu_slots}" ; then
    echo "#${_qbatch} -n ${_cpu_slots}" \
    >> "${_bat}"
  fi

  # Wall Clock Limit
  if ! empty "${_walltime}" ; then
    echo "#${_qbatch} -t ${_walltime}" \
    >> "${_bat}"
  fi

  # Standard Output FIle
  if ! empty "${_stdout}" ; then
    echo "#${_qbatch} -o ${_stdout}" \
    >> "${_bat}"
  fi

  # Standard Error File
  if ! empty "${_stderr}" ; then
    echo "#${_qbatch} -e ${_stderr}" \
    >> "${_bat}"
  fi

  # Copy Environment
  if [ "${_copy_env}"  = "yes" ] ; then
    echo "#${_qbatch} --export=ALL" \
    >> "${_bat}"
  fi

  # Event Notification
  # Email Address
  if ! empty "${_events}" && \
       test "${_events}" != "runprg" ; then

    echo "#${_qbatch} --mail-type=${_events}" \
    >> "${_bat}"

    if ! empty "${_mailto}" ; then
      echo "#${_qbatch} --mail-user=${_mailto}" \
      >> "${_bat}"
    fi
  fi

  # Job Name
  echo "#${_qbatch} --job-name ${_jobname}" \
  >> "${_bat}"

  # Job Restart
  if [ "${_restart}"  = "yes" ] ; then
    echo "#${_qbatch} --requeue" \
    >> "${_bat}"
  fi

  # Working Directory
  if ! empty "${_workdir}" ; then
    echo "#${_qbatch} --workdir=${_workdir}" \
    >> "${_bat}"
  fi

  # Resource Sharing
  if [ "${_exclusive}" = "yes" ] ; then
    echo "#${_qbatch} --exclusive" \
    >> "${_bat}"
  fi

  # Memory Size
  if ! empty "${_memory}" ; then
    echo "#${_qbatch} --mem-per-cpu=${_memory}M" \
    >> "${_bat}"
  fi

  # Account to charge
  if ! empty "${_account}" ; then
    echo "#${_qbatch} --account=${_account}" \
    >> "${_bat}"
  fi

  # Tasks Per Node
  if ! empty "${_tasks}" ; then
    echo "#${_qbatch} --ntasks-per-node=${_tasks}" \
    >> "${_bat}"
  fi

  # CPUs Per Task
  if ! empty "${_taskcpus}" ; then
    echo "#${_qbatch} --cpus-per-task=${_taskcpus}" \
    >> "${_bat}"
  fi

  # Job Dependency
  if ! empty "${_depend}" ; then
    echo "#${_qbatch} --depend=${_depend}" \
    >> "${_bat}"
  fi


  # Job Project
  if ! empty "${_project}" ; then
    echo "#${_qbatch} --wckey=${_project}" \
    >> "${_bat}"
  fi

  # Job host preference
  if ! empty "${_include_nodes}" ; then
    # TODO: filter spaces
    echo "#${_qbatch} --nodelist=${_include_nodes}" \
    >> "${_bat}"
  fi

  if ! empty "${_exclude_nodes}" ; then
    # TODO: filter spaces
    echo "#${_qbatch} --exclude=${_exclude_nodes}" \
    >> "${_bat}"
  fi

  # Quality Of Service
  if ! empty "${_quality}" ; then
    echo "#${_qbatch} --qos=${_quality}" \
    >> "${_bat}"
  fi

  # Job Arrays
  if ! empty "${_arrays}" ; then
    echo "#${_qbatch} --array=${_arrays}" \
    >> "${_bat}"
  fi

  # Generic Resources
  if ! empty "${_gpus}" ; then
    echo "#${_qbatch} --gres=gpu:${gpus}" \
    >> "${_bat}"
  fi

  # Licenses
  if ! empty "${_licenses}" ; then
    echo "#${_qbatch} --licenses=${_licenses}" \
    >> "${_bat}"
  fi

  # Begin Time
  if ! empty "${_begin}" ; then
    echo "#${_qbatch} --begin=${_begin}" \
    >> "${_bat}"
  fi

  # Constraints
  if ! empty "${_constraints}" ; then
    local _c
    for _c in ${_constraints} ; do
      echo "#${_qbatch} --constraint=${_c}" \
      >> "${_bat}"
    done
  fi

  # Options
  if ! empty "${_options}" ; then
    echo "#${_qbatch} ${_options}" \
    >> "${_bat}"
  fi
}


# ********************************************************
#    ______      ____
#   / ____/___  / / /_
#  / /   / __ \/ / __/
# / /___/ /_/ / / /_
# \____/\____/_/\__/
#
# ********************************************************

function que/slurm/run() {
  # mode
  local _mode=$1
  shift

  # queue
  local _queue=$1
  shift

  # nodes
  local _nodes=$1
  shift

  # sockets
  local _sckts=$1
  shift

  # cores
  local _cores=$1
  shift

  # tasks
  local _tasks=$((_cores*_sckts))

  # gpus
  local _gpus=$1
  if test ${_gpus} -lt 1 ; then
    _gpus=""
  else
    _gpus="--gres=gpu:${_gpus}"
  fi
  shift
  local _x=$*

  # cpus
  local _cpus=""
  if test "${_mode}" = "arr" ; then
    _cpus="-n ${_tasks}"
  else
    _cpus="-n ${_nodes} -B ${_sckts}:${_cores}"
  fi

  local _cmd="${que_slurm_run} -p ${_queue} ${_cpus} ${_gpus} ${_x}"
  answer "Show command?"
  if succeed $? ; then
    echo "${_cmd}"
    echo ""
  fi
  answer "Run interactive job?"
  if failed $? ; then $failure; fi
  ${_cmd}
}

function que/slurm/mail/sub() {
  echo "Job ${SLURM_JOB_ID} (${SLURM_JOB_NAME})"
}

function que/slurm/mail/msg() {
  local _m=""
  _m=$(date)
  if ! test -z "${SLURM_JOB_NUM_NODES}" ; then
    echo "${_m}\nRunning on ${SLURM_JOB_NUM_NODES} nodes"
  else
    echo "${_m}"
  fi
}
